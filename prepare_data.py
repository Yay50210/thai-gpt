import os
import torch
from transformers import AutoTokenizer
from tqdm import tqdm

# Folder containing your .txt summaries
DATA_DIR = "summaries"
SAVE_PATH = "data_thai_gpt.pt"

# 1Ô∏è‚É£ Load Thai tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "airesearch/wangchanberta-base-att-spm-uncased"
)

print("‚úÖ Tokenizer loaded:", tokenizer.name_or_path)

# 2Ô∏è‚É£ Read all text files and combine into one long string
texts = []
for fname in tqdm(os.listdir(DATA_DIR), desc="Reading files"):
    if fname.endswith(".txt"):
        path = os.path.join(DATA_DIR, fname)
        with open(path, "r", encoding="utf-8") as f:
            text = f.read().strip()
            if text:  # skip empty
                texts.append(text)

print(f"‚úÖ Loaded {len(texts)} documents")

# 3Ô∏è‚É£ Join with special separator (so sentences don't merge oddly)
full_text = "\n".join(texts)

# 4Ô∏è‚É£ Tokenize entire dataset
tokens = tokenizer.encode(full_text)

print(f"‚úÖ Tokenized length: {len(tokens):,} tokens")

# 5Ô∏è‚É£ Convert to tensor
data = torch.tensor(tokens, dtype=torch.long)

# 6Ô∏è‚É£ Split into train and validation sets (90/10)
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

torch.save((train_data, val_data), SAVE_PATH)
print(f"üíæ Saved dataset ‚Üí {SAVE_PATH}")